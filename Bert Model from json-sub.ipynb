{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4717f8c2",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea53fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b748d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:..........reetu\\\\Springboard course\\\\Project 2\\\\train1.csv')  \n",
    "test_data = pd.read_csv('C:\\\\.........\\\\reetu\\\\Springboard course\\\\Project 2\\\\test.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38152f",
   "metadata": {},
   "source": [
    "# Converting json format  \n",
    "(#https://pythonexamples.org/python-csv-to-json/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae935e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import json \n",
    "\n",
    "def csv_to_json(csvFilePath, jsonFilePath):\n",
    "    jsonArray = []\n",
    "      \n",
    "    #read csv file\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "        #load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csvf) \n",
    "\n",
    "        #convert each csv row into python dict\n",
    "        for row in csvReader: \n",
    "            #add this python dict to json array\n",
    "            jsonArray.append(row)\n",
    "  \n",
    "    #convert python jsonArray to JSON String and write to file\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "        jsonString = json.dumps(jsonArray, indent=4)\n",
    "        jsonf.write(jsonString)\n",
    "    #return jsonString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cfe0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFilePath = r'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\train1.csv'\n",
    "jsonFilePath = r'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\train1.json'\n",
    "jsonFilePath_train = csv_to_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275ffc8",
   "metadata": {},
   "source": [
    "# from test.csv to test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab8e1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFilePath = r'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\test.csv'\n",
    "jsonFilePath = r'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\test.json'\n",
    "jsonFilePath_test = csv_to_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fb6c1",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77f0b58a",
   "metadata": {},
   "source": [
    "Extraction\n",
    "The first step in preparing this data for fine-tuning is extracting our questions, contexts,\n",
    "and answers from the JSON files into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15aa0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(jsonFilePath):\n",
    "    with open(jsonFilePath, 'rb') as f:\n",
    "        data_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    answer_start = []\n",
    "    for group in data_dict:\n",
    "        groupKeys = group.keys()\n",
    "        #print(groupKeys)\n",
    "        if 'context' in groupKeys:\n",
    "            contexts.append(group['context'])\n",
    "        if 'question' in groupKeys:\n",
    "            questions.append(group['question'])\n",
    "        if 'answer_start' in groupKeys:\n",
    "            answer_start.append(group['answer_start'])\n",
    "        if 'answer_text' in groupKeys:\n",
    "            answers.append(group['answer_text'])\n",
    "    \n",
    "    return contexts, questions, answers, answer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ec2413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers, train_answer_start = read_json('C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\train1.json')\n",
    "test_contexts, test_questions, test_answers, test_answer_start= read_json('C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b01d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ज्वाला गुट्टा (जन्म: 7 सितंबर 1983; वर्धा, महाराष्ट्र) एक भारतीय बैडमिंटन खिलाडी हैं। \n",
      " प्रारंभिक जीवन \n",
      "ज्वाला गुट्टा का जन्म 7 सितंबर 1983 को वर्धा, महाराष्ट्र में हुआ था। उनके पिता एम. क्रांति तेलुगु और मां येलन चीन से हैं। उनकी मां येलन गुट्टा पहली बार 1977 में अपने दादा जी के साथ भारत आई थीं। ज्वाला गुट्टा की प्रारंभिक पढ़ाई हैदराबाद से हुई और यहीं से उन्होंने बैडमिंटन खेलना भी शुरू किया। \n",
      " कॅरियर \n",
      "10 साल की उम्र से ही ज्वाला गुट्टा ने एस.एम. आरिफ से ट्रेनिंग लेना शुरू कर दिया था। एस.एम. आरिफ भारत के जाने माने खेल प्रशिक्षक हैं जिन्हें द्रोणाचार्य अवार्ड से सम्मानित किया गया है। पहली बार 13 साल की उम्र में उन्होंने मिनी नेशनल बैडमिंटन चैंपियनशिप जीती थी। साल 2000 में ज्वाला गुट्टा ने 17 साल की उम्र में जूनियर नेशनल बैडमिंटन चैंपियनशिप जीती। इसी साल उन्होंने श्रुति कुरियन के साथ डबल्स में जोड़ी बनाते हुए महिलाओं के डबल्स जूनियर नेशनल बैडमिंटन चैंपियनशिप और सीनियर नेशनल बैडमिंटन चैंपियनशिप में जीत हासिल की। श्रुति कुरियन के साथ उनकी जोड़ी काफी लंबे समय तक चली। 2002 से 2008 तक लगातार सात बार ज्वाला गुट्टा ने महिलाओं के नेशनल युगल प्रतियोगिता में जीत हासिल की।[2]\n",
      "महिला डबल्स के साथ-साथ ज्वाला गुट्टा ने मिश्रित डबल्स में भी सफलता हासिल की और भारत की डबल्स में सबसे बेहतरीन खिलाड़ी बनीं।[3] 2010 कॉमनवेल्थ गेम्स में भी ज्वाला गुट्टा ने अपने पार्टनर अश्विनी पोनप्पा के साथ भारत के लिए स्वर्ण पदक जीता। कॉमनवेल्थ गेम्स के बाद से एक बार फिर ज्वाला गुट्टा भारतीय बैडमिंटन में चर्चा का विषय बन गई हैं।[4][5]\n",
      "ग्लासगो में आयोजित कॉमनवेल्थ गेम्स, 2014 में ज्वाला गुट्टा ने स्वर्ण पदक हासिल किया।\n",
      " व्यक्तिगत जीवन \n",
      "मैदान पर बाएं हाथ से तेज-तर्रार शॉट लगाने वाली ज्वाला निजी जिंदगी में भी काफी तेज और चर्चाओं में छाई रहती हैं। ज्वाला ने 2005 में बैडमिंटन खिलाड़ी चेतन आनंद से शादी की थी, 29 जून 2011 को उन्होंने अपने पति पूर्व बैडमिंटन खिलाड़ी चेतन आनंद से तलाक लिया है। चेतन आनंद भी एक बेहतरीन भारतीय बैडमिंटन खिलाड़ी हैं।\n",
      " फिल्मोग्राफी \n",
      "Gunde Jaari Gallanthayyinde[6] \n",
      "फुगली (2014)\n",
      " उपलब्धियां \n",
      "रिकॉर्ड 13 बार नेशनल बैडमिंटन चैंपियनशिप की विजेता। \n",
      "भारत की सबसे बेहतरीन डबल्स प्लेयर। \n",
      "साल 2011 में उन्हें “अर्जुन पुरस्कार” से सम्मानित किया गया। \n",
      "राष्ट्रमंडल खेल, 2014 (ग्लासगो) में स्वर्ण पदक जीता। \n",
      " चित्र दीर्घा \n",
      "\n",
      "\n",
      "वी दीजू और ज्वाला गुट्टा\n",
      "केबीसी के सेट पर सुशील कुमार, ज्वाला गुट्टा, लिएंडर पेस, श्रीसंत\n",
      "केबीसी के सेट पर सुशील कुमार, ज्वाला गुट्टा, लिएंडर पेस, श्रीसंत\n",
      "\n",
      " सन्दर्भ \n",
      "\n",
      " बाहरी कड़ियाँ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "श्रेणी:हिन्द की बेटियाँ\n",
      "श्रेणी:विकिपरियोजना हिन्द की बेटियाँ\n",
      "श्रेणी:भारत के खिलाड़ी\n",
      "श्रेणी:1983 में जन्मे लोग\n",
      "श्रेणी:जीवित लोग\n",
      "श्रेणी:भारतीय महिला बैडमिंटन खिलाड़ी\n",
      "श्रेणी:राष्ट्रमंडल खेलों के पदक प्राप्तकर्ता\n",
      "श्रेणी:महाराष्ट्र के लोग\n",
      "श्रेणी:बैडमिंटन खिलाड़ी\n"
     ]
    }
   ],
   "source": [
    "#print(train_contexts[0])\n",
    "print(test_contexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9213aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "மனித உடலில் எத்தனை எலும்புகள் உள்ளன?\n",
      "ज्वाला गुट्टा की माँ का नाम क्या है\n"
     ]
    }
   ],
   "source": [
    "print(train_questions[0])\n",
    "print(test_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ff937d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "print(type(len(train_answers[0])))\n",
    "#print(train_answers[0])\n",
    "\n",
    "print(train_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75991514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(train_answer_start[0])   # 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eb6d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "2\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(type(train_answer_start[0]))\n",
    "print(len(train_answer_start[0]))\n",
    "print(train_answer_start[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cde90",
   "metadata": {},
   "source": [
    "# Finding end value of end answer text\n",
    "find the start and end of our answer within the context — so we need to add an 'answer_end' value too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff7925d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# had index error: solved by https://www.stechies.com/typeerror-slice-indices-must-integers-none-have-index-metho/#:~:text=slice%20indices%20must%20be%20integers%20or%20None%20or,provide%20value%20other%20than%20an%20integer%20to%20it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932b8df",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92e8c4",
   "metadata": {},
   "source": [
    "This concatenated version is stored within the input_ids attribute of our Encoding object. the data is stored as BERT-readable token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d53b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "# initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "# tokenize\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_contexts, test_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "711cfac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] [UNK] வளரநத மனிதனுடைய [UNK] பினவரும 206 ( [UNK] [UNK] பகுதிகளாகக கருதபபடடால 208 ) [UNK] [UNK] [UNK]. [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]. [UNK], மிகக [UNK] [UNK] மனிதரகளில, [UNK] மேலதிக விலா [UNK] ( [UNK] ) [UNK] [UNK] மேலதிகமான [UNK] [UNK] [UNK] ; [UNK] சில [UNK] தனி [UNK] கருதாவிடின, [UNK] [UNK] [UNK] ; [UNK] ( 3 - 5 ) குயிலலகு [UNK] சேரநது 26 [UNK] [UNK] 33 [UNK] கருதபபடலாம. மனித [UNK] 22 [UNK] ( காதுச [UNK] தவிர ) [UNK] ; [UNK] [UNK] [UNK] ( cranium ) [UNK] 14 முக [UNK] ( facial bones ) பிரிககபபடடுளளன. ( தடிதத [UNK] [UNK] படததில [UNK] [UNK] [UNK]. ) [UNK] [UNK] ( 8 ) 1 [UNK] ( frontal bone ) 2 [UNK] ( parietal bone ) ( 2 ) 3 [UNK] ( temporal bone ) ( 2 ) 4 பிடர [UNK] ( occipital bone ) [UNK] [UNK] ( sphenoid bone ) [UNK] ( ethmoid bone ) முக [UNK] ( 14 ) 7 [UNK] [UNK] ( mandible ) 6 [UNK] [UNK] ( maxilla ) ( 2 ) [UNK] ( palatine bone ) ( 2 ) 5 கனன [UNK] ( zygomatic bone ) ( 2 ) 9 நாசி [UNK] ( nasal bone ) ( 2 ) [UNK] [UNK] ( lacrimal bone ) ( 2 ) [UNK] சுவர [UNK] ( vomer ) [UNK] [UNK] [UNK] ( inferior nasal conchae ) ( 2 ) நடுககாதுகளில ( 6 ) : சமமடடியுரு ( malleus ) படடையுரு ( incus ) [UNK] ( stapes ) [UNK] ( 1 ) : [UNK] [UNK] ( நாவடி [UNK] ) ( hyoid ) தோள படடையில ( 4 ) : 25. [UNK] [UNK] ( clavicle ) 29. தோள [UNK] ( scapula ) [SEP] மனித [UNK] [UNK] [UNK] [UNK]? [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54480fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b595f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 2 tokens.\n"
     ]
    }
   ],
   "source": [
    "print('The input has a total of {:} tokens.'.format(len(train_encodings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b19acd",
   "metadata": {},
   "source": [
    "# Initializing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d22fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts, answer_start, encodings):\n",
    "    answers_end = []\n",
    "    answers_start = []\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    # loop through each answer-context pair\n",
    "    for answer, context, ans_start in zip(answers, contexts, answer_start):\n",
    "        answers_start.append(ans_start)\n",
    " \n",
    "        # gold_text: expected answer to find in context\n",
    "        gold_text = answer # len(answers)                                              \n",
    "   \n",
    "        # changing the type from str to int\n",
    "        start_idx = int(ans_start)\n",
    "        #print(start_idx)\n",
    "        \n",
    "        # finding end index by adding len of goldtext to start index\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "        #print(end_idx)\n",
    "        \n",
    "        # sometimes answers are off by a character or two\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            # if the answer is not off :)\n",
    "            #print('in')\n",
    "            answers_end.append(end_idx)\n",
    "            #answers_end + end_idx\n",
    "        else:\n",
    "            #print('out')\n",
    "            # this means the answer is off by 1-2 tokens\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                    answers_start = start_idx - n\n",
    "                    print(end_idx - n)\n",
    "                    answers_end.append(end_idx - n)\n",
    "                   # answers_end + (end_idx - n)\n",
    "        #Breaking out of loop after first iteration. Remove below line for complete processing\n",
    "        #print('ans_start', answers_start)\n",
    "        #break\n",
    "     \n",
    "    for i in range(len(answers_start)):\n",
    "      \n",
    "        start_positions.append(encodings.char_to_token(i, int(answers_start[i]))) \n",
    "        end_positions.append(encodings.char_to_token(i, int(answers_end[i])))\n",
    "       \n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        # end position cannot be found, char_to_token found space, so shift position until found\n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, int(answers_end[i] - shift))\n",
    "            shift += 1\n",
    "    # update our encodings object with the new token-based start/end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "       \n",
    "        # append start/end token position using char_to_token method\n",
    "    #encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "        #print(start_positions)\n",
    "    return answers_end, start_positions,end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecad87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and apply the function to our two answer lists\n",
    "answers_end, start_positions, end_positions = add_end_idx(train_answers, train_contexts, train_answer_start, train_encodings)\n",
    "#print(start_positions)\n",
    "#print('-----------------------------------------------------')\n",
    "#print(end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b23f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b85eb636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  1319, 29871, 29876, 29870, 29876,  1317, 29856, 29856, 29876,\n",
      "         1006,  1319, 29863, 29867,  1024,  1021,  1338, 29877, 29859, 29865,\n",
      "        29869,  3172,  1025,  1335, 29869, 29862, 29876,  1010,  1331, 29875,\n",
      "        29876, 29869, 29876, 29873, 29856, 29869,  1007,  1314, 29851,  1330,\n",
      "        29876, 29869, 29859, 29878, 29868,  1329, 29857, 29867, 29877, 29856,\n",
      "        29863,  1316, 29877, 29870, 29876, 29857, 29878,  1339,  1344,  1328,\n",
      "        29869, 29876, 29869, 29866, 29877, 29851,  1319, 29878, 29871, 29863,\n",
      "         1319, 29871, 29876, 29870, 29876,  1317, 29856, 29856, 29876,  1315,\n",
      "        29876,  1319, 29863, 29867,  1021,  1338, 29877, 29859, 29865, 29869,\n",
      "         3172,  1315, 29879,  1335, 29869, 29862, 29876,  1010,  1331, 29875,\n",
      "        29876, 29869, 29876, 29873, 29856, 29869,  1331,  1339, 29848,  1324,\n",
      "        29876,  1344,  1313, 29863, 29851,  1328, 29877, 29859, 29876,  1314,\n",
      "        29867,  1012,  1315, 29869, 29876, 29859, 29877,  1323, 29870, 29853,\n",
      "          100,  1331, 29876,  1332, 29870, 29863,  1318, 29878, 29863,  1338,\n",
      "         1339,  1344,  1313, 29863, 29851, 29878,  1331, 29876,  1332, 29870,\n",
      "        29863,  1317, 29856, 29856, 29876,  1328, 29875, 29870, 29878,  1329,\n",
      "        29876, 29869,  3355,  1331,  1311, 29864, 29863,  1325, 29876, 29861,\n",
      "        29876,  1319, 29878,  1315,  1338, 29876, 29860,  1330, 29876, 29869,\n",
      "        29859,   100,  1324, 29878,  1344,  1319, 29871, 29876, 29870, 29876,\n",
      "         1317, 29856, 29856, 29876,  1315, 29878,  1328, 29869, 29876, 29869,\n",
      "        29866, 29877, 29851,   100,  1339, 29861, 29869, 29876, 29865, 29876,\n",
      "        29861,  1338,   100,   100,  1332, 29875, 29878,  1338,  1313, 29863,\n",
      "        29875, 29879, 29863,  1329, 29857, 29867, 29877, 29856, 29863,  1316,\n",
      "        29870, 29863, 29876,  1330, 29878,  1336, 29869,  1315, 29877, 29868,\n",
      "        29876,  1344,  1315, 29869, 29877, 29868, 29869,  2184,  1338, 29876,\n",
      "        29870,  1315, 29878,  1313, 29867, 29869,  1338,  1339, 29878,  1319,\n",
      "        29871, 29876, 29870, 29876,  1317, 29856, 29856, 29876,  1327,  1314,\n",
      "        29874,  1012,  1314, 29867,  1012,   100,  1338,  1320, 29869, 29863,\n",
      "        29877, 29853,  1334, 29863, 29876,  1336, 29869,  1315, 29869,  1325,\n",
      "        29877, 29868, 29876,  1324, 29876,  1344,  1314, 29874,  1012,  1314,\n",
      "        29867,  1012,   100,  1330, 29876, 29869, 29859,  1315,  1319, 29876,\n",
      "        29863,  1331, 29876, 29863,  1316, 29870,  1328, 29869, 29872, 29877,\n",
      "        29851, 29873, 29851,  1339,  1319, 29877, 29863, 29875,  1325, 29869,\n",
      "        29879, 29858, 29876, 29854, 29876, 29869, 29868,  1311, 29871, 29876,\n",
      "        29869, 29857,  1338,  1338, 29867, 29867, 29876, 29863, 29877, 29859,\n",
      "         1315, 29877, 29868, 29876,  1317, 29868, 29876,  1339,  1344,  1328,\n",
      "        29875, 29870, 29878,  1329, 29876, 29869,  2410,  1338, 29876, 29870,\n",
      "         1315, 29878,  1313, 29867, 29869,  1331,  1313, 29863, 29875, 29879,\n",
      "        29863,  1331, 29877, 29863, 29878,  1327, 29872, 29863, 29870,  1329,\n",
      "        29857, 29867, 29877, 29856, 29863,  1318, 29864, 29877, 29868, 29863,\n",
      "        29872, 29877, 29864,  1319, 29878, 29859, 29878,  1324, 29878,  1344,\n",
      "         1338, 29876, 29870,  2456,  1331,  1319, 29871, 29876, 29870, 29876,\n",
      "         1317, 29856, 29856, 29876,  1327,  2459,  1338, 29876, 29870,  1315,\n",
      "        29878,  1313, 29867, 29869,  1331,  1319, 29863, 29877, 29868, 29869,\n",
      "         1327, 29872, 29863, 29870,  1329, 29857, 29867, 29877, 29856, 29863,\n",
      "         1318, 29864, 29877, 29868, 29863, 29872, 29877, 29864,  1319, 29878,\n",
      "        29859, 29878,  1344,   100,  1338, 29876, 29870,  1313, 29863, 29875,\n",
      "        29879, 29863,  1336, 29869, 29859, 29877,  1315, 29869, 29877, 29868,\n",
      "        29863,  1315,  1338, 29876, 29860,  1321, 29865, 29870,   102,  1319,\n",
      "        29871, 29876, 29870, 29876,  1317, 29856, 29856, 29876,  1315, 29878,\n",
      "         1331, 29876,  1315, 29876,  1327, 29876, 29867,  1315, 29868, 29876,\n",
      "         1339,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# build datasets for both our train and test sets\n",
    "train_dataset = Dataset(train_encodings)\n",
    "test_dataset = Dataset(test_encodings)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d07bc",
   "metadata": {},
   "source": [
    "# Fine-Tuning\n",
    "Our data is ready for the model. we need to set the PyTorch environment, initialize the DataLoader which we will be using to load data during training — and finally, begin fine-tuning our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bddd9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "313a7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "335c326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████| 70/70 [53:48<00:00, 35.21s/it, loss=2.2]\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████| 70/70 [49:00<00:00, 34.74s/it, loss=3.46]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 70/70 [48:55<00:00, 34.89s/it, loss=2.5]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# move model over to detected device\n",
    "model.to(device)\n",
    "# activate training mode of model\n",
    "model.train()\n",
    "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# initialize data loader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    # setup loop (we use tqdm for the progress bar)\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all the tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        # extract loss\n",
    "        loss = outputs[0]\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70592813",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ec7c5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom\\\\vocab.txt',\n",
       " 'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom\\\\added_tokens.json')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802c1d2",
   "metadata": {},
   "source": [
    "# Reloading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6715637",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom' \n",
    "#model_path = 'C:\\\\Users\\\\vishd\\\\Desktop\\\\reetu\\\\Springboard course\\\\Project 2\\\\distilbert-custom'\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb9bad",
   "metadata": {},
   "source": [
    "# Extracting the start and end token position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c1c57dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can access the start_logits and end_logits tensors and perform an argmax function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b93ea3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1,  56,  89,   1,   2,  70,   1,   1, 410,   1])\n",
      "tensor([491, 504, 495, 468, 468, 489,  14,   8, 485,   8])\n"
     ]
    }
   ],
   "source": [
    "start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "print(start_pred)\n",
    "end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "print(end_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b748a",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57887a2",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "The simplest metric we can measure here is accuracy — more specifically known as exact match (EM).\n",
    "To calculate the EM of each batch, we take the sum of the number of matches per batch — and divide by the total. We do this with PyTorch like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bfa86aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69.8000)\n"
     ]
    }
   ],
   "source": [
    "acc = start_pred.sum() / len(start_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f7fe8cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 14, 408,  25, 235, 512, 512,   1, 104,  32, 512])\n",
      "tensor([ 17, 418,  31, 242, 493, 481,   6, 104,  40, 486])\n",
      "tensor([ 14,  23,  25, 235,   1,  20,  72, 164,  32, 112])\n",
      "tensor([ 17,  31,  24, 475, 493, 481, 490, 104,  40, 486])\n",
      "0.5000000149011612\n"
     ]
    }
   ],
   "source": [
    "start_true = batch['start_positions'].to(device)\n",
    "print(start_true)\n",
    "        #start_positions = batch['start_positions'].to(device)\n",
    "end_true = batch['end_positions'].to(device)\n",
    "print(end_true)\n",
    "        # make predictions\n",
    "start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "print(start_pred)\n",
    "end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "print(end_pred)\n",
    "acc = []\n",
    "acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "acc = sum(acc)/len(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34db8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# move model over to detected device\n",
    "model.to(device)\n",
    "# activate training mode of model\n",
    "model.train()\n",
    "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# switch model out of training mode\n",
    "model.eval()\n",
    "# initialize testing set data loader\n",
    "test_loader = DataLoader(train_dataset, batch_size=16)\n",
    "# initialize list to store accuracies\n",
    "acc = []\n",
    "# loop through batches\n",
    "for batch in test_loader:\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # we will use true positions for accuracy calc\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        #start_positions = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        # make predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # pull prediction tensors out and argmax to get predicted tokens\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        # calculate accuracy for both and append to accuracy list\n",
    "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "# calculate average accuracy in total\n",
    "acc = sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3a0ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4959821430700166\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6da20d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://careerkarma.com/blog/python-typeerror-str-object-cannot-be-interpreted-as-an-integer/\n",
    "#https://careerkarma.com/blog/python-valueerror-invalid-literal-for-int-with-base-10/#:~:text=The%20Python%20ValueError%3A%20invalid%20literal%20for%20int%20%28%29,%28%29%20to%20convert%20your%20number%20to%20an%20integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32087e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-fine-tune-a-q-a-transformer-86f91ec92997\n",
    "#https://huggingface.co/bert-base-multilingual-uncased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
